# -*- coding: utf-8 -*-
"""test_fits_data_processing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jM7a58zVnQ_8W76Z_lyxCsf4NrxoF4Wa
"""

pip install astropy pandas matplotlib seaborn pandas-profiling fpdf requests

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.io import fits
import requests
from io import BytesIO
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Step 1: Load the .fits file from GitHub
url = 'https://github.com/Adrita-Khan/AstroPhotoZ/raw/main/Datasets/Stripe_82X/final_stripe_82X_photozs_brown.fits'
response = requests.get(url)

# Ensure successful request
response.raise_for_status()

fits_file = BytesIO(response.content)

# Step 2: Open the FITS file
with fits.open(fits_file) as hdul:
    # Step 3: Inspect the FITS structure (Header and Data)
    header = hdul[0].header
    data = hdul[1].data

    # Print header and some data
    print(header)
    print(data[:5])  # Print the first 5 rows of data

    # Step 4: Convert FITS data to a Pandas DataFrame
    df = pd.DataFrame(data)

# Step 5: Basic EDA
print(df.info())  # Data types and missing values
print(df.describe())  # Statistical summary of numerical columns

# Step 6: Visualize some basic plots
# Distribution of numerical columns
df.hist(bins=50, figsize=(20, 15))
plt.tight_layout()
plt.show()

# Step 7: Detecting missing values
missing = df.isnull().sum()
missing_percentage = (missing / df.shape[0]) * 100
print("Missing values (in percentage):\n", missing_percentage)

# Step 8: Handle categorical features (if any)
categorical_cols = df.select_dtypes(include='object').columns
if not categorical_cols.empty:
    for col in categorical_cols:
        print(f"Label encoding for column: {col}")
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])

# Step 9: Generate Additional Plots for Insights

# Pairplot for a subset of features
sns.pairplot(df.select_dtypes(include=np.number).iloc[:, :5], height=2.5)

# Save the pairplot to a file
pairplot_filename = "pairplot.png"  # Define the file name for saving the plot
plt.tight_layout()
plt.savefig(pairplot_filename)  # Save the pairplot to a file

# Display the pairplot
plt.show()

# Optionally close the plot if you no longer need it
plt.close()

# Step 3: Inspect the FITS structure (Header and Data)
header = hdul[0].header
data = hdul[1].data

# Print FITS file header information
print("FITS File Header Information:")
print(header)

# Step 5: Basic EDA - Data Summary (Descriptive Statistics)
print("\nData Summary (Descriptive Statistics):")
print(df.describe())  # Statistical summary of numerical columns

# Step 7: Detecting missing values
missing = df.isnull().sum()
missing_percentage = (missing / df.shape[0]) * 100
print("\nMissing Values (in percentage):")
print(missing_percentage)

# Step 11: Data Normalization or Scaling
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Example: Min-Max Scaling (normalization)
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df.select_dtypes(include=np.number)), columns=df.select_dtypes(include=np.number).columns)

# Check the scaled data
print(df_scaled.head())

# Step 12: Imputation for Missing Values
# Use the median for numerical columns
df.fillna(df.median(), inplace=True)

# Alternatively, for categorical columns:
# df['column_name'].fillna(df['column_name'].mode()[0], inplace=True)

# For categorical data, fill missing values with the mode (most frequent)
for col in df.select_dtypes(include='object').columns:
    df[col].fillna(df[col].mode()[0], inplace=True)

# Step 11: Feature Scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_features = scaler.fit_transform(df.select_dtypes(include=np.number))  # Scaling only numerical features
df_scaled = pd.DataFrame(scaled_features, columns=df.select_dtypes(include=np.number).columns)

from sklearn.decomposition import PCA
pca = PCA(n_components=2)  # Reduce to 2 components for visualization
df_pca = pca.fit_transform(df.select_dtypes(include=np.number))
plt.scatter(df_pca[:, 0], df_pca[:, 1])
plt.title("PCA of Dataset")
plt.show()

from sklearn.decomposition import PCA

# Perform PCA with 2 components
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df.select_dtypes(include=np.number))

# Convert PCA result to DataFrame with column names 'PCA1' and 'PCA2'
df_pca_df = pd.DataFrame(df_pca, columns=['PCA1', 'PCA2'])

# Optionally, join the PCA components back to the original DataFrame
df_with_pca = df.join(df_pca_df)

# Display the new DataFrame with PCA components
print(df_with_pca.head())

# Save the DataFrame with PCA components to a CSV file
#df_with_pca.to_csv('df_with_pca.csv', index=False)

# Provide a link to download the CSV file
#from google.colab import files
#files.download('df_with_pca.csv')

df_cleaned = df.dropna()

# Optionally, check the shape before and after dropping rows
print(f"Original data shape: {df.shape}")
print(f"Cleaned data shape: {df_cleaned.shape}")

# Select only numerical columns (excluding non-numeric columns like headers or identifiers)
df_numeric = df.select_dtypes(include=[np.number])

# Drop rows that do not contain numerical data in all columns (except the heading column)
df_cleaned = df[df_numeric.notna().all(axis=1)]

# Optionally, check the shape before and after dropping rows
print(f"Original data shape: {df.shape}")
print(f"Cleaned data shape: {df_cleaned.shape}")

# Define the list of relevant columns for estimating photometric redshift
relevant_columns = [
    'MAG_FUV', 'MAGERR_FUV', 'MAG_NUV', 'MAGERR_NUV',
    'U', 'UERR', 'G', 'GERR', 'R', 'RERR', 'I', 'IERR', 'Z', 'ZERR',
    'J', 'JERR', 'H', 'HERR', 'K', 'KERR', 'W1', 'W1ERR', 'W2', 'W2ERR',
    'PHOTOZ', 'PHOTOZ_BEST68_LOW', 'PHOTOZ_BEST68_HIGH', 'PHOTOZ_ML',
    'PHOTOZ_ML68_LOW', 'PHOTOZ_ML68_HIGH',
    'REDSHIFT', 'REDSHIFT_ERR', 'REDSHIFT_SOURCE',
    'SOFT_FLUX', 'HARD_FLUX', 'FULL_FLUX',
    'CHI_BEST', 'MOD_BEST', 'EXTLAW_BEST', 'EBV_BEST'
]

# Retrieve only the relevant columns from the DataFrame
df_relevant = df[relevant_columns]

# Optionally, check the first few rows of the filtered DataFrame
print(df_relevant.head())

# Basic summary statistics
print("\nSummary Statistics:\n", df_relevant.describe())

import seaborn as sns
import matplotlib.pyplot as plt

# Heatmap to visualize missing values
plt.figure(figsize=(10, 6))
sns.heatmap(df_relevant.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Data Heatmap")
plt.show()

from scipy.stats import zscore

# Compute Z-scores for numerical columns
z_scores = np.abs(zscore(df_relevant.select_dtypes(include=np.number)))
outliers = (z_scores > 3)  # Typically, a Z-score greater than 3 indicates an outlier

# Check how many outliers there are
print("\nNumber of outliers in each column:\n", np.sum(outliers, axis=0))

# Check for constant columns (columns with only one unique value)
constant_columns = [col for col in df_relevant.columns if df_relevant[col].nunique() == 1]
print("\nConstant Columns (with only one unique value):", constant_columns)

# Check for duplicate rows
duplicates = df_relevant[df_relevant.duplicated()]
print("\nDuplicate Rows:\n", duplicates)

# Calculate and display the correlation matrix
correlation_matrix = df_relevant.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title("Correlation Matrix")
plt.show()

# Importing necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Prepare your data (X = features, y = target)
X = df_relevant.drop(['REDSHIFT'], axis=1)  # Drop the target column (REDSHIFT)
y = df_relevant['REDSHIFT']  # Target is the REDSHIFT column

# Ensure X has only numerical data
X = X.select_dtypes(include=np.number)

# Split data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error
r2 = r2_score(y_test, y_pred)  # R-squared (Coefficient of Determination)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Optionally, compare predicted vs. actual redshift values
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, color='blue', alpha=0.6)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.xlabel('True Redshift')
plt.ylabel('Predicted Redshift')
plt.title('True vs Predicted Redshift')
plt.show()

train_r2 = model.score(X_train, y_train)
print(f"Training R-squared: {train_r2}")

feature_importances = model.feature_importances_
feature_names = X.columns
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
})
importance_df = importance_df.sort_values(by='Importance', ascending=False)
print(importance_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Get feature importances from the trained model
feature_importances = model.feature_importances_

# Create a DataFrame for easier handling
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
})

# Sort the DataFrame by importance in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')
plt.title("Feature Importance for Photometric Redshift Prediction")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.show()


# -*- coding: utf-8 -*-
"""Photometric_Redshift_Dataset_Exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vDzcdqeHzT4_Nvm8Y-5gHyh4GxMpNo40
"""

# Step 1: Import Required Libraries
import pandas as pd

# Step 2: Define the Raw URL of the Dataset
url = "https://raw.githubusercontent.com/Adrita-Khan/AstroPhotoZ/main/Datasets/SDSS/star_classification.csv"

# Step 3: Load the Dataset
data = pd.read_csv(url)

# Step 4: Preview the Data
print("First 5 rows of the dataset:")
print(data.head())

# Step 5: Display Column Names
print("\nColumn names in the dataset:")
print(data.columns)

# Step 6: Display Dataset Information
print("\nDataset information:")
print(data.info())

# Step 7: Display Descriptive Statistics
print("\nDescriptive statistics of the dataset:")
print(data.describe())

# Step 8: Save the Dataset Locally in Colab (Optional)
data.to_csv('star_classification_local.csv', index=False)
print("\nDataset saved locally as 'star_classification_local.csv'.")

# Step 9: Download the Dataset (Optional)
#from google.colab import files
#files.download('star_classification_local.csv')

# Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Step 2: Load the Dataset
url = "https://raw.githubusercontent.com/Adrita-Khan/AstroPhotoZ/main/Datasets/SDSS/star_classification.csv"
data = pd.read_csv(url)

# Step 3: Basic Information
print("First 5 rows of the dataset:")
print(data.head())

print("\nDataset shape (rows, columns):")
print(data.shape)

print("\nDataset information:")
print(data.info())

print("\nMissing values in the dataset:")
print(data.isnull().sum())

print("\nDescriptive statistics of numerical columns:")
print(data.describe())

# Step 4: Univariate Analysis
# Plot the distribution of the target variable (if any)
if 'class' in data.columns:
    print("\nClass distribution:")
    print(data['class'].value_counts())
    plt.figure(figsize=(8, 5))
    sns.countplot(x='class', data=data)
    plt.title('Distribution of Classes')
    plt.xlabel('Class')
    plt.ylabel('Count')
    plt.show()

# Histograms for numerical features
numerical_features = data.select_dtypes(include=['float64', 'int64']).columns
data[numerical_features].hist(figsize=(15, 10), bins=20)
plt.suptitle('Histograms of Numerical Features')
plt.show()

# Step 5: Correlation Analysis
# Filter only numeric columns
numeric_data = data.select_dtypes(include=['float64', 'int64'])

if not numeric_data.empty:
    plt.figure(figsize=(12, 8))
    correlation_matrix = numeric_data.corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
    plt.title('Correlation Heatmap')
    plt.show()
else:
    print("No numeric columns available for correlation analysis.")

# Step 6: Pairplot
# Pairplot for the first 5 numerical features
if not numeric_data.empty:
    sns.pairplot(data[numerical_features].iloc[:, :5])
    plt.suptitle('Pairplot of Numerical Features', y=1.02)
    plt.show()

# Step 7: Outlier Detection
# Boxplots for numerical features
for feature in numerical_features:
    plt.figure(figsize=(10, 5))
    sns.boxplot(x=data[feature])
    plt.title(f'Boxplot of {feature}')
    plt.show()

# Step 8: Missing Values Handling (if needed)
if data.isnull().sum().sum() > 0:
    print("\nHandling missing values by filling with mean (for numerical columns):")
    data.fillna(data.mean(), inplace=True)
    print("Missing values handled.")

# Step 9: Categorical Data Analysis
categorical_features = data.select_dtypes(include=['object']).columns
for feature in categorical_features:
    plt.figure(figsize=(8, 5))
    sns.countplot(y=feature, data=data, order=data[feature].value_counts().index)
    plt.title(f'Distribution of {feature}')
    plt.xlabel('Count')
    plt.ylabel(feature)
    plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Enable inline plotting for Jupyter notebooks (if using one)
# %matplotlib inline

# Step 2: Define the Raw URL of the Dataset
url = "https://raw.githubusercontent.com/Adrita-Khan/AstroPhotoZ/main/Datasets/SDSS/star_classification.csv"

# Step 3: Load the Dataset into a Pandas DataFrame
try:
    data = pd.read_csv(url)
    print("Dataset loaded successfully.")
except Exception as e:
    print(f"Error loading dataset: {e}")

# Step 4: Display the First Few Rows of the Dataset
print("\nFirst 5 rows of the dataset:")
print(data.head())

# Step 5: Get Basic Information about the Dataset
print("\nDataset Information:")
print(data.info())

# Step 6: Check for Missing Values
print("\nMissing Values in Each Column:")
print(data.isnull().sum())

# Step 7: Descriptive Statistics
print("\nDescriptive Statistics:")
print(data.describe())

# Step 8: Analyze the Distribution of Each Feature
# List of numerical columns (assuming all except 'class' are numerical)
numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()

# Plot histograms for numerical features
print("\nPlotting Histograms for Numerical Features...")
data[numerical_cols].hist(bins=30, figsize=(20, 15), layout=(5, 4))
plt.tight_layout()
plt.show()

# Step 9: Boxplots to Detect Outliers
print("\nPlotting Boxplots for Numerical Features...")
plt.figure(figsize=(20, 15))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(5, 4, i)
    sns.boxplot(y=data[col])
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()

# Step 10: Correlation Matrix and Heatmap
print("\nCorrelation Matrix:")
# Select only numeric columns to avoid errors
numeric_data = data.select_dtypes(include=[np.number])

# Check if 'class' is numeric; if not, exclude it
if 'class' in numeric_data.columns:
    corr_matrix = numeric_data.corr()
else:
    corr_matrix = numeric_data.corr()

print(corr_matrix)

print("\nPlotting Correlation Heatmap...")
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

# Step 11: Pair Plot to Visualize Relationships Between Features
# Note: Pair plots can be time-consuming for large datasets
print("\nPlotting Pair Plot (This may take a while)...")
# To speed up, you can sample the data if it's large
sampled_data = data.sample(n=500, random_state=42) if len(data) > 500 else data
sns.pairplot(sampled_data, hue='class', diag_kind='kde', corner=True)
plt.show()

# Step 12: Analyze the Target Variable Distribution
print("\nDistribution of Target Variable 'class':")
print(data['class'].value_counts())

print("\nPlotting Target Variable Distribution...")
sns.countplot(x='class', data=data)
plt.title("Class Distribution")
plt.show()

# Step 13: Feature Importance (Optional)
# If you plan to perform modeling, you might want to look at feature importance.
# Here's an example using a simple model.
from sklearn.ensemble import RandomForestClassifier

# Encode target variable if it's categorical
if data['class'].dtype == 'object':
    data['class_encoded'] = data['class'].astype('category').cat.codes
    target_col = 'class_encoded'
else:
    target_col = 'class'

# Define features and target
X = data.drop(['class'], axis=1)

# If 'class_encoded' was created, include it only as the target
if 'class_encoded' in X.columns:
    X = X.drop(['class_encoded'], axis=1)

y = data[target_col]

# Check for non-numeric columns in features
non_numeric_features = X.select_dtypes(exclude=[np.number]).columns.tolist()
if non_numeric_features:
    print("\nNon-numeric features found in predictors. Encoding them...")
    # Example: One-Hot Encoding
    X = pd.get_dummies(X, drop_first=True)

# Initialize Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model
rf.fit(X, y)

# Get feature importances
importances = rf.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

print("\nFeature Importances:")
print(feature_importance_df)

# Plot Feature Importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title("Feature Importances from Random Forest")
plt.show()

# Step 14: Handle Missing Values (If Any)
# Example: Fill numerical missing values with median
if data.isnull().sum().sum() > 0:
    print("\nHandling Missing Values...")
    for col in numerical_cols:
        if data[col].isnull().sum() > 0:
            median = data[col].median()
            data[col].fillna(median, inplace=True)
            print(f"Filled missing values in {col} with median value {median}.")
    # Optionally handle categorical missing values
    for col in data.select_dtypes(include=['object', 'category']).columns:
        if data[col].isnull().sum() > 0:
            mode = data[col].mode()[0]
            data[col].fillna(mode, inplace=True)
            print(f"Filled missing values in {col} with mode value '{mode}'.")
else:
    print("\nNo missing values to handle.")

# Step 15: Save the Cleaned Data (Optional)
# data.to_csv('cleaned_star_classification.csv', index=False)
# print("\nCleaned data saved to 'cleaned_star_classification.csv'.")